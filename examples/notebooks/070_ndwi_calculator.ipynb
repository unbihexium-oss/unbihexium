{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NDWI Calculator\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/unbihexium-oss/unbihexium/blob/main/examples/notebooks/070_ndwi_calculator.ipynb)\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-View_Source-181717?logo=github)](https://github.com/unbihexium-oss/unbihexium)\n",
    "[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n",
    "\n",
    "---\n",
    "\n",
    "**Author**: Unbihexium OSS Foundation  \n",
    "**Version**: 1.0.0  \n",
    "**Last Updated**: 2025-12-21  \n",
    "**Task Type**: `regression`\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#1-introduction)\n",
    "2. [Model Overview](#2-model-overview)\n",
    "3. [Environment Setup](#3-environment-setup)\n",
    "4. [Model Loading](#4-model-loading)\n",
    "5. [Variant Comparison](#5-variant-comparison)\n",
    "6. [Inference Pipeline](#6-inference-pipeline)\n",
    "7. [Performance Benchmarks](#7-performance-benchmarks)\n",
    "8. [Integration Examples](#8-integration-examples)\n",
    "9. [Best Practices](#9-best-practices)\n",
    "10. [References](#10-references)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Normalized Difference Water Index calculation.\n",
    "\n",
    "This notebook provides a comprehensive guide to using the Unbihexium library's `ndwi_calculator` model family. The model is available in four variants (tiny, base, large, mega) to accommodate different computational constraints and accuracy requirements.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Task**: Regression\n",
    "- **Variants**: 4 (tiny, base, large, mega)\n",
    "- **Formats**: ONNX and PyTorch (.pt)\n",
    "- **Framework**: Unbihexium Model Zoo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Overview\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "The `ndwi_calculator` model is designed for:\n",
    "\n",
    "- Primary application: Normalized Difference Water Index calculation\n",
    "- Integration with geospatial workflows\n",
    "- Batch processing of satellite imagery\n",
    "- Real-time inference for operational systems\n",
    "\n",
    "### Technical Specifications\n",
    "\n",
    "| Variant | Resolution | Channels | Parameters | Use Case |\n",
    "|---------|------------|----------|------------|----------|\n",
    "| tiny | 32x32 | 16 | ~17K | Ultra-lightweight for edge devices and rapid prototyping |\n",
    "| base | 64x64 | 64 | ~268K | Balanced performance for production deployments |\n",
    "| large | 128x128 | 128 | ~1M | High accuracy for demanding applications |\n",
    "| mega | 256x256 | 256 | ~4M | Maximum precision for research and critical tasks |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Environment Setup\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Python 3.10 or higher\n",
    "- 8 GB RAM minimum (16 GB recommended for large/mega variants)\n",
    "- CUDA-compatible GPU (optional, for accelerated inference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git lfs install\n",
    "# !git lfs pull\n",
    "\n",
    "# Install required packages\n",
    "# Uncomment the following lines if running in a fresh environment\n",
    "\n",
    "# !pip install unbihexium\n",
    "# !pip install onnxruntime  # or onnxruntime-gpu for GPU acceleration\n",
    "# !pip install torch torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify installation\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "try:\n",
    "    import unbihexium\n",
    "    print(f\"Unbihexium version: {unbihexium.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Unbihexium not installed. Run: pip install unbihexium\")\n",
    "\n",
    "try:\n",
    "    import onnxruntime as ort\n",
    "    print(f\"ONNX Runtime version: {ort.__version__}\")\n",
    "    print(f\"Available providers: {ort.get_available_providers()}\")\n",
    "except ImportError:\n",
    "    print(\"ONNX Runtime not installed. Run: pip install onnxruntime\")\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "except ImportError:\n",
    "    print(\"PyTorch not installed. Run: pip install torch\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Loading\n",
    "\n",
    "### 4.1 Using Unbihexium Model Zoo\n",
    "\n",
    "The recommended approach is to use the Unbihexium Model Zoo API for automatic model management.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Define model paths for all variants\n",
    "MODEL_ID = \"ndwi_calculator\"\n",
    "\n",
    "BASE_DIR = Path().resolve()\n",
    "MODEL_ZOO = None\n",
    "for parent in [BASE_DIR] + list(BASE_DIR.parents):\n",
    "    candidate = parent / \"model_zoo\" / \"assets\"\n",
    "    if candidate.exists():\n",
    "        MODEL_ZOO = candidate\n",
    "        break\n",
    "\n",
    "if MODEL_ZOO is None:\n",
    "    raise FileNotFoundError(\"Could not find 'model_zoo/assets' in parent directories.\")\n",
    "\n",
    "VARIANTS = [\"tiny\", \"base\", \"large\", \"mega\"]\n",
    "\n",
    "# Load configuration for each variant\n",
    "configs = {}\n",
    "\n",
    "for variant in VARIANTS:\n",
    "    variant_folder = MODEL_ZOO / variant\n",
    "    if not variant_folder.exists():\n",
    "        print(f\"{variant.upper()}: Variant folder not found: {variant_folder}\")\n",
    "        continue\n",
    "\n",
    "    model_folders = [f for f in variant_folder.iterdir() if f.is_dir()]\n",
    "    if not model_folders:\n",
    "        print(f\"{variant.upper()}: No models found in {variant_folder}\")\n",
    "        continue\n",
    "\n",
    "    # Assuming the first folder matches the pattern or is the correct one\n",
    "    model_folder = model_folders[0]\n",
    "    config_file = model_folder / \"config.json\"\n",
    "    if config_file.exists():\n",
    "        with open(config_file) as f:\n",
    "            configs[variant] = json.load(f)\n",
    "            print(f\"{variant.upper()}: {configs[variant]}\")\n",
    "    else:\n",
    "        print(f\"{variant.upper()}: config.json not found in {model_folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Direct ONNX Loading\n",
    "\n",
    "For production deployments, ONNX format provides cross-platform compatibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "# Load ONNX models for all variants\n",
    "onnx_sessions = {}\n",
    "\n",
    "for variant in VARIANTS:\n",
    "    model_path = MODEL_ZOO / variant / f\"{MODEL_ID}_{variant}\" / \"model.onnx\"\n",
    "    if model_path.exists():\n",
    "        try:\n",
    "            session = ort.InferenceSession(\n",
    "                str(model_path),\n",
    "                providers=['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "            )\n",
    "            onnx_sessions[variant] = session\n",
    "            \n",
    "            # Print model info\n",
    "            input_info = session.get_inputs()[0]\n",
    "            output_info = session.get_outputs()[0]\n",
    "            print(f\"{variant.upper()} ONNX loaded:\")\n",
    "            print(f\"  Input: {input_info.name} {input_info.shape}\")\n",
    "            print(f\"  Output: {output_info.name} {output_info.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"{variant.upper()}: Failed to load - {e}\")\n",
    "    else:\n",
    "        print(f\"{variant.upper()}: ONNX model not found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 PyTorch Loading\n",
    "\n",
    "For research and fine-tuning, PyTorch format provides full model access.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load PyTorch models\n",
    "pt_models = {}\n",
    "\n",
    "for variant in VARIANTS:\n",
    "    model_path = MODEL_ZOO / variant / f\"{MODEL_ID}_{variant}\" / \"model.pt\"\n",
    "    if model_path.exists():\n",
    "        try:\n",
    "            model = torch.jit.load(str(model_path), map_location='cpu')\n",
    "            model.eval()\n",
    "            pt_models[variant] = model\n",
    "            \n",
    "            # Count parameters\n",
    "            param_count = sum(p.numel() for p in model.parameters())\n",
    "            print(f\"{variant.upper()} PyTorch loaded: {param_count:,} parameters\")\n",
    "        except Exception as e:\n",
    "            print(f\"{variant.upper()}: Failed to load - {e}\")\n",
    "    else:\n",
    "        print(f\"{variant.upper()}: PyTorch model not found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Variant Comparison\n",
    "\n",
    "Compare the four model variants across key metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Compare model sizes and configurations\n",
    "comparison_data = []\n",
    "\n",
    "for variant in VARIANTS:\n",
    "    onnx_path = MODEL_ZOO / variant / f\"{MODEL_ID}_{variant}\" / \"model.onnx\"\n",
    "    pt_path = MODEL_ZOO / variant / f\"{MODEL_ID}_{variant}\" / \"model.pt\"\n",
    "    \n",
    "    row = {\"Variant\": variant.upper()}\n",
    "    \n",
    "    if onnx_path.exists():\n",
    "        row[\"ONNX Size (MB)\"] = round(os.path.getsize(onnx_path) / (1024 * 1024), 2)\n",
    "    if pt_path.exists():\n",
    "        row[\"PT Size (MB)\"] = round(os.path.getsize(pt_path) / (1024 * 1024), 2)\n",
    "    if variant in configs:\n",
    "        row[\"Parameters\"] = configs[variant].get(\"params\", \"N/A\")\n",
    "        row[\"Resolution\"] = configs[variant].get(\"resolution\", \"N/A\")\n",
    "    \n",
    "    comparison_data.append(row)\n",
    "\n",
    "# Display comparison table\n",
    "try:\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(comparison_data)\n",
    "    print(df.to_string(index=False))\n",
    "except ImportError:\n",
    "    for row in comparison_data:\n",
    "        print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inference Pipeline\n",
    "\n",
    "### 6.1 Input Preparation\n",
    "\n",
    "Prepare input data according to model requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(resolution: int, channels: int = 3, batch_size: int = 1):\n",
    "    \"\"\"\n",
    "    Prepare synthetic input tensor for inference.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    resolution : int\n",
    "        Spatial resolution (width and height)\n",
    "    channels : int\n",
    "        Number of input channels (default: 3 for RGB)\n",
    "    batch_size : int\n",
    "        Batch size for inference\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Input tensor of shape (batch_size, channels, resolution, resolution)\n",
    "    \"\"\"\n",
    "    # Generate synthetic input (replace with actual data in production)\n",
    "    return np.random.rand(batch_size, channels, resolution, resolution).astype(np.float32)\n",
    "\n",
    "# Resolution mapping for each variant\n",
    "RESOLUTIONS = {\n",
    "    \"tiny\": 32,\n",
    "    \"base\": 64,\n",
    "    \"large\": 128,\n",
    "    \"mega\": 256\n",
    "}\n",
    "\n",
    "# Prepare inputs for all variants\n",
    "inputs = {}\n",
    "for variant, res in RESOLUTIONS.items():\n",
    "    inputs[variant] = prepare_input(res)\n",
    "    print(f\"{variant.upper()} input shape: {inputs[variant].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 ONNX Inference\n",
    "\n",
    "Run inference using ONNX Runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ONNX inference for all variants\n",
    "onnx_outputs = {}\n",
    "\n",
    "for variant, session in onnx_sessions.items():\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    input_data = inputs[variant]\n",
    "    \n",
    "    # Run inference\n",
    "    output = session.run(None, {input_name: input_data})\n",
    "    onnx_outputs[variant] = output[0]\n",
    "    \n",
    "    print(f\"{variant.upper()} ONNX output shape: {output[0].shape}\")\n",
    "    print(f\"  Min: {output[0].min():.4f}, Max: {output[0].max():.4f}, Mean: {output[0].mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 PyTorch Inference\n",
    "\n",
    "Run inference using PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PyTorch inference for all variants\n",
    "pt_outputs = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for variant, model in pt_models.items():\n",
    "        input_tensor = torch.from_numpy(inputs[variant])\n",
    "        \n",
    "        # Run inference\n",
    "        output = model(input_tensor)\n",
    "        pt_outputs[variant] = output.numpy()\n",
    "        \n",
    "        print(f\"{variant.upper()} PyTorch output shape: {output.shape}\")\n",
    "        print(f\"  Min: {output.min():.4f}, Max: {output.max():.4f}, Mean: {output.mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Benchmarks\n",
    "\n",
    "Measure inference time for each variant and framework.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark(func, n_runs: int = 10, warmup: int = 3):\n",
    "    \"\"\"\n",
    "    Benchmark a function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    func : callable\n",
    "        Function to benchmark\n",
    "    n_runs : int\n",
    "        Number of timed runs\n",
    "    warmup : int\n",
    "        Number of warmup runs\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (mean_time_ms, std_time_ms)\n",
    "    \"\"\"\n",
    "    # Warmup\n",
    "    for _ in range(warmup):\n",
    "        func()\n",
    "    \n",
    "    # Timed runs\n",
    "    times = []\n",
    "    for _ in range(n_runs):\n",
    "        start = time.perf_counter()\n",
    "        func()\n",
    "        end = time.perf_counter()\n",
    "        times.append((end - start) * 1000)  # Convert to ms\n",
    "    \n",
    "    return np.mean(times), np.std(times)\n",
    "\n",
    "# Benchmark ONNX inference\n",
    "print(\"ONNX Runtime Performance:\")\n",
    "print(\"-\" * 50)\n",
    "onnx_benchmark = {}\n",
    "for variant, session in onnx_sessions.items():\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    input_data = inputs[variant]\n",
    "    \n",
    "    mean_ms, std_ms = benchmark(lambda: session.run(None, {input_name: input_data}))\n",
    "    onnx_benchmark[variant] = mean_ms\n",
    "    print(f\"{variant.upper():6} | {mean_ms:8.2f} ms +/- {std_ms:.2f} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark PyTorch inference\n",
    "print(\"PyTorch Performance:\")\n",
    "print(\"-\" * 50)\n",
    "pt_benchmark = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for variant, model in pt_models.items():\n",
    "        input_tensor = torch.from_numpy(inputs[variant])\n",
    "        \n",
    "        mean_ms, std_ms = benchmark(lambda: model(input_tensor))\n",
    "        pt_benchmark[variant] = mean_ms\n",
    "        print(f\"{variant.upper():6} | {mean_ms:8.2f} ms +/- {std_ms:.2f} ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Integration Examples\n",
    "\n",
    "### 8.1 Batch Processing\n",
    "\n",
    "Process multiple images in a single batch for improved throughput.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_inference(session, images: list, batch_size: int = 4):\n",
    "    \"\"\"\n",
    "    Process images in batches.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    session : ort.InferenceSession\n",
    "        ONNX inference session\n",
    "    images : list\n",
    "        List of input images (numpy arrays)\n",
    "    batch_size : int\n",
    "        Batch size for processing\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of outputs\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    \n",
    "    for i in range(0, len(images), batch_size):\n",
    "        batch = np.stack(images[i:i+batch_size])\n",
    "        output = session.run(None, {input_name: batch})\n",
    "        results.extend(output[0])\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example: Process 8 images with the base variant\n",
    "if \"base\" in onnx_sessions:\n",
    "    sample_images = [prepare_input(64)[0] for _ in range(8)]\n",
    "    batch_results = batch_inference(onnx_sessions[\"base\"], sample_images, batch_size=4)\n",
    "    print(f\"Processed {len(sample_images)} images, got {len(batch_results)} outputs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Model Selection Strategy\n",
    "\n",
    "Choose the appropriate variant based on requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_variant(\n",
    "    max_latency_ms: float = None,\n",
    "    max_memory_mb: float = None,\n",
    "    min_resolution: int = None,\n",
    "    prefer_accuracy: bool = True\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Select the best model variant based on constraints.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    max_latency_ms : float, optional\n",
    "        Maximum acceptable latency in milliseconds\n",
    "    max_memory_mb : float, optional\n",
    "        Maximum model size in megabytes\n",
    "    min_resolution : int, optional\n",
    "        Minimum required input resolution\n",
    "    prefer_accuracy : bool\n",
    "        If True, prefer larger models when constraints allow\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Recommended variant name\n",
    "    \"\"\"\n",
    "    # Priority order based on preference\n",
    "    variants = [\"mega\", \"large\", \"base\", \"tiny\"] if prefer_accuracy else [\"tiny\", \"base\", \"large\", \"mega\"]\n",
    "    \n",
    "    for variant in variants:\n",
    "        # Check latency constraint\n",
    "        if max_latency_ms and variant in onnx_benchmark:\n",
    "            if onnx_benchmark[variant] > max_latency_ms:\n",
    "                continue\n",
    "        \n",
    "        # Check resolution constraint\n",
    "        if min_resolution and RESOLUTIONS[variant] < min_resolution:\n",
    "            continue\n",
    "        \n",
    "        return variant\n",
    "    \n",
    "    return \"base\"  # Default fallback\n",
    "\n",
    "# Example usage\n",
    "recommended = select_variant(max_latency_ms=50, prefer_accuracy=True)\n",
    "print(f\"Recommended variant for <50ms latency: {recommended.upper()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Best Practices\n",
    "\n",
    "### Variant Selection Guidelines\n",
    "\n",
    "| Scenario | Recommended Variant | Rationale |\n",
    "|----------|--------------------|-----------|\n",
    "| Edge deployment (IoT, drones) | tiny | Minimal memory and compute |\n",
    "| Production API service | base | Balanced performance |\n",
    "| High-accuracy batch processing | large | Better accuracy, acceptable latency |\n",
    "| Research and validation | mega | Maximum precision |\n",
    "\n",
    "### Performance Optimization\n",
    "\n",
    "1. **Use ONNX for Production**: ONNX Runtime provides optimized execution across platforms.\n",
    "2. **Enable GPU Acceleration**: Use `CUDAExecutionProvider` for NVIDIA GPUs.\n",
    "3. **Batch Processing**: Increase batch size to improve GPU utilization.\n",
    "4. **Model Caching**: Cache loaded models to avoid repeated loading overhead.\n",
    "5. **Input Preprocessing**: Use vectorized operations for input preparation.\n",
    "\n",
    "### Memory Management\n",
    "\n",
    "```python\n",
    "# Clear GPU memory after inference\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. References\n",
    "\n",
    "### Documentation\n",
    "\n",
    "- [Unbihexium Documentation](https://unbihexium-oss.github.io/unbihexium/)\n",
    "- [Model Zoo Reference](https://github.com/unbihexium-oss/unbihexium/tree/main/model_zoo)\n",
    "- [ONNX Runtime Documentation](https://onnxruntime.ai/docs/)\n",
    "- [PyTorch Documentation](https://pytorch.org/docs/)\n",
    "\n",
    "### Related Notebooks\n",
    "\n",
    "- See the [notebooks index](./README.md) for all available tutorials.\n",
    "\n",
    "---\n",
    "\n",
    "**License**: Apache-2.0  \n",
    "**Copyright**: 2025 Unbihexium OSS Foundation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "colab": {
   "provenance": [],
   "collapsed_sections": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}